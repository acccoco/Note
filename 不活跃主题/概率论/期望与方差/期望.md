
### 期望的定义
**离散随机变量的期望**：
随机变量的期望定义如下：
$$E[X] = \sum_x x\  p_X(x)
$$

**连续随机变量的期望**：
$$E[X] = \int_{-\infty}^{\infty} x\ f_X(x)\ dx
$$

其中，$f_X(x)\ dx$  可以理解为随机变量 $X$  位于区间 $[x,\ x+dx]$  上的概率。
随机变量函数的期望为：
$$E[g(X)] = \int_{-\infty}^{\infty} g(x)\ f_X(x)\ dx
$$


### g(X) 的期望
如果有两个随机变量 $X,\ Y$  ，关系如下：
![[不活跃主题/概率论/期望与方差/_attachments/1643033088573-b22a3c46-41a2-4cd0-8373-74f0e4775c9b.png | 208]]
求 $Y$  的期望。
有以下两种计算方法：

- 如果已知 $Y$  的 PMF，那么：

$$E[Y] = \sum_y y\ p_Y(y)
$$

- 如果只知道 $X$  的 PMF，那么：

$$E[Y] = \sum_x g(x) \ p_X(x)
$$

> 注：期望的函数并不等于函数的期望：
> $$E[g(X)] \ne g(E[X])$$


**随机变量线性函数的期望**：
$$
E[\alpha X + \beta] = \alpha E[X] + \beta
$$


### 离散 g(X, Y) 的期望
随机变量函数的均值：
$$E[g(X,\ Y)] = \sum_x \sum_y g(x,\ y) \cdot p_{X,\ Y}(x,\ y)
$$

多个随机变量的线性组合，其均值为：（非常简单，不证明）
$$E[\alpha X + \beta Y] = \alpha E[X] + \beta E[Y]
$$

如果两个随机变量**独立**，那么两者乘积的均值为：
$$\begin{array}{rl}
	E[XY] & = \sum_x \sum_y xy \cdot p_{X,\ Y}(x,\ y) \\
	      & = \sum_x \sum_y xy \cdot p_X(x) \cdot p_Y(y) \\
	      & = E[X] \cdot E[Y]
\end{array}$$

如果 $X,\ Y$  相互独立，那么相关的函数 $g(X),\ h(Y)$  也应当独立，类似的有：
$$E[g(X)\cdot h(Y)] = E[g(X)] \cdot E[h(Y)]
$$


### 连续 g(X, Y) 的期望
根据定义来计算：
$$E[g(X,\ Y)] = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x,\ y)\ f_{X,\ Y}(x,\ y)\ dxdy
$$

对比离散的多个随机变量函数的期望：
$$E[g(X,\ Y)] = \sum_x \sum_y p_{X,\ Y}(x, y)
$$
